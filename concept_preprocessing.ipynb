{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Ypp8tuSylx",
        "outputId": "70bb86ab-6835-41a0-9d72-873d9db6b1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import mmap\n",
        "from math import sqrt\n",
        "import random\n",
        "import json\n",
        "import spacy\n",
        "import spacy.cli\n",
        "import string\n",
        "import nltk\n",
        "import csv\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import auc\n",
        "import gensim\n",
        "from gensim.matutils import softcossim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import multiprocessing\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "root_path = os.path.join(os.getcwd(), 'data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0UcSdXA6cg",
        "outputId": "1695d787-d19f-4699-ccaf-bdabae73b063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-15 17:40:31,295 : INFO : loading projection weights from /content/gdrive/My Drive/OA_proj/data/PubMed-and-PMC-w2v.bin\n",
            "2022-04-15 17:41:57,382 : INFO : loaded (4087446, 200) matrix from /content/gdrive/My Drive/OA_proj/data/PubMed-and-PMC-w2v.bin\n",
            "2022-04-15 17:41:57,385 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ],
      "source": [
        "model = KeyedVectors.load_word2vec_format(os.path.join(root_path, 'PubMed-and-PMC-w2v.bin'), binary=True)\n",
        "model.init_sims(replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "code",
        "id": "TyPbYvk8SNl5"
      },
      "outputs": [],
      "source": [
        "# ########################## Function Timing ###############################\n",
        "## Just add @timing before any functions (def)\n",
        "\n",
        "def timing(f):\n",
        "    def wrap(*args, **kwargs):\n",
        "        time1 = time.time()\n",
        "        ret = f(*args, **kwargs)\n",
        "        time2 = time.time()\n",
        "        print('{:s} function took {:.4f} (s)'.format(f.__name__, (time2-time1)))\n",
        "        return ret\n",
        "    return wrap\n",
        "\n",
        "############################################################################\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable = ['tagger', 'parser', 'ner'])\n",
        "stop_words = stopwords.words('english')\n",
        "punctuations = string.punctuation\n",
        "\n",
        "def single_text_clean(doc): \n",
        "    doc = nlp(doc)\n",
        "    tokens = [tok.lemma_.strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]\n",
        "    return tokens\n",
        "\n",
        "@timing\n",
        "def ontology2json(rfile, delim, filenames, w2v_model): \n",
        "    dicts = []\n",
        "    for i, rf in enumerate(rfile):\n",
        "        infile = open(rf,'rb+')\n",
        "        jsondict = {}\n",
        "        contents = infile.read()\n",
        "        soup = BeautifulSoup(contents,'lxml-xml')\n",
        "        classes = soup.find_all('Class')\n",
        "        desc = soup.find_all('Description')\n",
        "\n",
        "        for cl in tqdm(classes):\n",
        "            if cl.has_attr('rdf:about'):\n",
        "                labels = cl.find_all('label')\n",
        "                relsy = cl.find_all('hasRelatedSynonym')\n",
        "                subc = cl.find_all('subClassOf')\n",
        "                # desc = cl.find_all('Description')\n",
        "                subcc = cl.find_all('someValuesFrom')\n",
        "                # subcc_all = cl.find_all('allValuesFrom')\n",
        "                hasdef = cl.find_all('hasDefinition')\n",
        "\n",
        "                s1 = cl.attrs['rdf:about']\n",
        "                ss = s1[s1.index(delim) + 1:].replace('_','').replace('-','').lower() # concept code\n",
        "                jsondict[ss] = {'label': [], 'embed': [], 'alignable': 1, 'related': []}\n",
        "                for lb in labels:\n",
        "                    lab = lb.get_text().replace(',','').replace('_',' ').replace('-',' ').replace('/',' or ').lower()\n",
        "                    tok_list = single_text_clean(lab)\n",
        "                    emb_list = []\n",
        "                    jsondict[ss]['label'].append(lab)\n",
        "                    if tok_list:\n",
        "                        for tok in tok_list:\n",
        "                            if tok in w2v_model.vocab:\n",
        "                                emb_list.append(w2v_model[tok])\n",
        "                        if not emb_list:\n",
        "                            jsondict[ss]['embed'].append(np.zeros(w2v_model.vector_size).tolist())\n",
        "                        else:\n",
        "                            jsondict[ss]['embed'].append(np.mean(emb_list, axis=0).tolist())\n",
        "                    else:\n",
        "                        jsondict[ss]['embed'].append(np.zeros(w2v_model.vector_size).tolist())\n",
        "\n",
        "              #  if relsy_len != 0 or subc_len != 0:\n",
        "                for rs in relsy:\n",
        "                    # if rs.has_attr('rdf:resource'):\n",
        "                    r1 = rs.attrs['rdf:resource']\n",
        "                    jsondict[ss]['related'].append({'code': r1[r1.index(delim) + 1:].replace('_','').lower(), 'reltype':'hasRelatedSynonym'}) \n",
        "\n",
        "                for sc in subc:\n",
        "                    if sc.has_attr('rdf:resource'):\n",
        "                        if sc.attrs['rdf:resource'] != 'http://www.w3.org/2002/07/owl#Thing':\n",
        "                            st = sc.attrs['rdf:resource']\n",
        "                            jsondict[ss]['related'].append({'code': st[st.index(delim) + 1:].replace('_','').lower(), 'reltype':'subClassOf'}) # .replace('-',' ')\n",
        "                \n",
        "                for scc in subcc:\n",
        "                    if scc.has_attr('rdf:resource'):\n",
        "                        st = scc.attrs['rdf:resource']\n",
        "                        jsondict[ss]['related'].append({'code': st[st.index(delim) + 1:].replace('_','').lower(), 'reltype':'RestrictedsubClassOf'}) # .replace('-',' ')\n",
        "                \n",
        "                for hd in hasdef:\n",
        "                    hdf = hd.attrs['rdf:resource']\n",
        "                    jsondict[ss]['related'].append({'code': hdf[hdf.index(delim) + 1:].replace('_','').lower(), 'reltype':'hasDefinition'}) \n",
        "                                        \n",
        "        for ds in desc:\n",
        "            dscon = ds.attrs['rdf:about']\n",
        "            dsid = dscon[dscon.index(delim) + 1:].replace('_','').lower()\n",
        "            jsondict[dsid] = {'label': [], 'embed': [], 'alignable': 0, 'related': []}\n",
        "            desc_labels = ds.find_all('label')\n",
        "            for ds_label in desc_labels:\n",
        "                lab = ds_label.get_text().replace(',','').replace('_',' ').replace('-',' ').replace('/',' or ').lower()\n",
        "                tok_list = single_text_clean(lab)\n",
        "                emb_list = []\n",
        "                jsondict[dsid]['label'].append(lab)\n",
        "                if tok_list:\n",
        "                    for tok in tok_list:\n",
        "                        if tok in w2v_model.vocab:\n",
        "                            emb_list.append(w2v_model[tok])\n",
        "                    if not emb_list:\n",
        "                        jsondict[dsid]['embed'].append(np.zeros(w2v_model.vector_size).tolist())\n",
        "                    else:    \n",
        "                        jsondict[dsid]['embed'].append(np.mean(emb_list, axis=0).tolist())\n",
        "                else:\n",
        "                    jsondict[dsid]['embed'].append(np.zeros(w2v_model.vector_size).tolist())\n",
        "            \n",
        "        infile.close()\n",
        "        dicts.append(jsondict) \n",
        "        with open(os.path.join(root_path, filenames[i]), 'w+') as fp:\n",
        "            json.dump(jsondict, fp, indent=4)\n",
        "    return dicts\n",
        "\n",
        "@timing\n",
        "def ontology2json_ref(rfile, delim, filename):\n",
        "    outfile = open(os.path.join(root_path, filename), 'w+', newline='')\n",
        "    out_writer = csv.writer(outfile, delimiter=',')\n",
        "\n",
        "    for rf in rfile:\n",
        "        infile = open(rf,'rb+')\n",
        "        contents = infile.read()\n",
        "        soup = BeautifulSoup(contents,'lxml-xml')\n",
        "        maps = soup.find_all('Cell')\n",
        "        \n",
        "        for mp in tqdm(maps):\n",
        "            en1 = mp.find_all('entity1')\n",
        "            en2 = mp.find_all('entity2')\n",
        "            row = []\n",
        "            sr1 = en1[0].attrs['rdf:resource']\n",
        "            row.append(sr1[sr1.index(delim) + 1:].replace('_', '').replace('-','').lower())\n",
        "            sr2 = en2[0].attrs['rdf:resource']\n",
        "            row.append(sr2[sr2.index(delim) + 1:].replace('_', '').replace('-','').lower())\n",
        "            out_writer.writerow(row)\n",
        "\n",
        "        infile.close()\n",
        "    outfile.close()\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhiydpJSBFiD",
        "outputId": "1ee3cabc-5a36-479f-fca0-b112e9d2da5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3304/3304 [00:01<00:00, 3185.40it/s]\n",
            "100%|██████████| 2744/2744 [00:00<00:00, 4329.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ontology2json function took 14.1137 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1516/1516 [00:00<00:00, 35331.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ontology2json_ref function took 1.0247 (s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "readfile = []\n",
        "readfile.append(os.path.join(root_path, 'anatomy', 'human.owl'))\n",
        "readfile.append(os.path.join(root_path, 'anatomy', 'mouse.owl'))\n",
        "filenames = ['human.json','mouse.json']\n",
        "jdict = ontology2json(readfile, '#', filenames, model)\n",
        "readfile = []\n",
        "readfile.append(os.path.join(root_path, 'anatomy', 'reference.rdf'))\n",
        "ontology2json_ref(readfile, '#', 'ref.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Ont_Align.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}